{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import machine learning methods\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Import package for SMOTE\n",
    "import imblearn\n",
    "\n",
    "# Turn warnings off to keep notebook clean\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read CSV into Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/processed_stroke.csv')\n",
    "# Make all data 'float' type and drop ID\n",
    "data = data.astype(float)\n",
    "data.drop('id', axis=1, inplace=True) # Remove ID column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Record number in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record number in each class\n",
    "number_positive_stroke = np.sum(data['stroke'] == 1)\n",
    "number_negative_stroke = np.sum(data['stroke'] == 0)\n",
    "\n",
    "print (f\"Positives : {number_positive_stroke}\")\n",
    "print (f\"Negatives : {number_negative_stroke}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide into X (features) and y (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('stroke',axis=1) # X = all 'data' except the 'stroke' column\n",
    "y = data['stroke'] # y = 'stroke' column from 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show examples from the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_data(X_train, X_test):\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = StandardScaler() \n",
    "\n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_std=sc.transform(X_train)\n",
    "    test_std=sc.transform(X_test)\n",
    "    \n",
    "    return train_std, test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std, X_test_std = standardise_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Logistic Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_std,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use fitted model to make predictions on training and test set data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict training and test set labels\n",
    "y_pred_train = model.predict(X_train_std)\n",
    "y_pred_test = model.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train = np.mean(y_pred_train == y_train)\n",
    "accuracy_test = np.mean(y_pred_test == y_test)\n",
    "\n",
    "print (f'Accuracy of predicting training data = {accuracy_train:0.3f}')\n",
    "print (f'Accuracy of predicting test data = {accuracy_test:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_synthetic_data_smote(X, y, number_of_samples=[1000,1000]):\n",
    "    \"\"\"\n",
    "    Synthetic data generation for two classes.\n",
    "        \n",
    "    Inputs\n",
    "    ------\n",
    "    original_data: X, y numpy arrays (y should have label 0 and 1)\n",
    "    number_of_samples: number of samples to generate (list for y=0, y=1)\n",
    "    (Note - number_of_samples has default of 1000 samples for each class\n",
    "    if no numbers are specified at the point of calling the function)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_synthetic: NumPy array\n",
    "    y_synthetic: NumPy array\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # import SMOTE from imblearn so we can use it\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    \n",
    "    # Count instances in each class\n",
    "    count_label_0 = np.sum(y==0)\n",
    "    count_label_1 = np.sum(y==1)\n",
    "    \n",
    "    # SMOTE requires final class counts; add current counts to required counts\n",
    "    # (which are passed into the function)\n",
    "    n_class_0 = number_of_samples[0] + count_label_0\n",
    "    n_class_1 = number_of_samples[1] + count_label_1\n",
    "\n",
    "    # Use SMOTE to sample data points.  The number of points that we pass over\n",
    "    # to SMOTE is calculated above (the number of synthetic data samples we\n",
    "    # want, which we passed into the function + the counts from the original\n",
    "    # data).  This tells SMOTE how many TOTAL data points are needed (original\n",
    "    # + synthetic) for each class.  It then uses the original data to generate\n",
    "    # new synthetic data points.\n",
    "    # For example, imagine our original data has 100 samples for class 0 and 50\n",
    "    # for class 1, and we tell SMOTE we want 100 synthetic data points for \n",
    "    # class 0 and 150 synthetic data points for class 1.  We tell SMOTE that we\n",
    "    # need a total of 200 data points for class 0 (100 original + 100 synthetic)\n",
    "    # and 200 data points for class 1 (50 original + 150 synthetic).  It will\n",
    "    # then fill those data points by taking the original data (which will fill\n",
    "    # up the first 100 \"slots\" for class 0, and the first 50 \"slots\" for class 1)\n",
    "    # and then use these original data points to sample new synthetic data points\n",
    "    # to fill the remaining \"slots\" in each class.\n",
    "    X_resampled, y_resampled = SMOTE(\n",
    "        sampling_strategy = {0:n_class_0, 1:n_class_1}).fit_resample(X, y)\n",
    "\n",
    "    # Get just the additional (synthetic) data points.  By using len(X) for the\n",
    "    # X (input feature) data, and len(y) for the y (output label) data, we skip\n",
    "    # the original data, and just start from the newly created synthetic data,\n",
    "    # generated by SMOTE (above)\n",
    "    X_synthetic = X_resampled[len(X):]\n",
    "    y_synthetic = y_resampled[len(y):]\n",
    "                                                                   \n",
    "    return X_synthetic, y_synthetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate raw synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts of classes from y_train\n",
    "unique, original_frequency = np.unique(y_train, return_counts = True)\n",
    "required_smote_count = list(original_frequency * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function we wrote above to generate and extract the synthetic data\n",
    "X_synthetic, y_synthetic = make_synthetic_data_smote(\n",
    "        X_train, y_train, number_of_samples=required_smote_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_synthetic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare lists of categorical, integer and binary features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get full list of column names (the names of our features)\n",
    "X_col_names = list(X_train)\n",
    "\n",
    "# Set categorical one-hots cols using common prefix\n",
    "categorical = ['gender_', 'work_type_', 'smoking_status_']\n",
    "\n",
    "one_hot_cols = []\n",
    "for col in categorical:\n",
    "    one_hot_cols.append([x for x in X_col_names if x[0:len(col)] == col])\n",
    "    \n",
    "# Set integer columns\n",
    "integer_cols = ['age']\n",
    "\n",
    "# Set binary columns\n",
    "binary_cols = ['hypertension',\n",
    "               'heart_disease',\n",
    "               'ever_married',\n",
    "               'residence_type_rural']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to process raw synthetic categorical data to one-hot encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(x):\n",
    "    \"\"\"\n",
    "    Takes a list/array/series and turns it into a one-hot encoded\n",
    "    list/array series, by setting 1 for highest value and 0 for all \n",
    "    others\n",
    "    \n",
    "    \"\"\"\n",
    "    # Get argmax (this returns the index of the highest values in\n",
    "    # the list / array / series passed in to the function)\n",
    "    highest = np.argmax(x)\n",
    "    # Set all values to zero (just multiply all values by 0)\n",
    "    x *= 0.0\n",
    "    # Set the value that was found to be the highest to 1, by\n",
    "    # using the index we found using argmax above\n",
    "    x[highest] = 1.0\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process raw synthetic data and show a sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set y_label\n",
    "y_label = 'stroke'\n",
    "\n",
    "# Create a data frame with id to store the synthetic data\n",
    "synth_df = pd.DataFrame()\n",
    "\n",
    "# Transfer X values to the new DataFrame\n",
    "synth_df=pd.concat([synth_df, \n",
    "                    pd.DataFrame(X_synthetic, columns=X_col_names)],\n",
    "                    axis=1)\n",
    "\n",
    "# Make columns (that need to be) one hot encoded using the\n",
    "# function we wrote above, using the raw synthetic data\n",
    "for one_hot_list in one_hot_cols:    \n",
    "    for index, row in synth_df.iterrows():\n",
    "        x = row[one_hot_list]\n",
    "        x_one_hot = make_one_hot(x)\n",
    "        row[x_one_hot.index]= x_one_hot.values\n",
    "\n",
    "# Make integer as necessary by rounding the raw synthetic data\n",
    "for col in integer_cols:\n",
    "    synth_df[col] = synth_df[col].round(0)\n",
    "\n",
    "# Round binary cols and clip so values under 0 or above 1\n",
    "# are set to 0 and 1 respectively (this won't happen with\n",
    "# SMOTE, as it will only sample between the two points (so \n",
    "# points sampled between binary points will always be \n",
    "# between 0 and 1) but it can happen with other methods)\n",
    "for col in binary_cols:\n",
    "    synth_df[col] = np.clip(synth_df[col],0,1).round(0)\n",
    "    \n",
    "# Add y data with a label\n",
    "y_list = list(y_synthetic)\n",
    "synth_df[y_label] = y_list\n",
    "\n",
    "# Shuffle data\n",
    "synth_df = synth_df.sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find nearest original data point to each synthetic data point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise synthetic data (based on real training data)\n",
    "X_train_std, X_synth_std = standardise_data(X_train, X_synthetic)\n",
    "\n",
    "# Get ALL real X data (combine standardised training + test data)\n",
    "# We do this because we need to check for duplicates / very close\n",
    "# values in all of the real data we've got\n",
    "X_real_std = np.concatenate([X_train_std, X_test_std], axis=0)\n",
    "  \n",
    "# Use SciKitLearn neighbors.NearestNeighbors to find nearest neighbour\n",
    "# to each data point. First, we fit to the real standardised data \n",
    "# (all of it, train + test set).  Then we can give it the synthetic data\n",
    "# and ask it to give us the cartesian distance and ID of its nearest\n",
    "# real world data point neighbour for each synthetic data point.\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nn = NearestNeighbors(n_neighbors=1, algorithm='auto').fit(X_real_std)\n",
    "dists, idxs = nn.kneighbors(X_synth_std)\n",
    "\n",
    "# Store the index and ids (indices) in the synthetic data DataFrame\n",
    "# Flatten just reduces something in more than 1 dimension down to\n",
    "# 1 dimension (eg a list of lists becomes a single list)\n",
    "synth_df['distance_to_closest_real'] = list(dists.flatten())\n",
    "synth_df['closest_X_real_row_index'] = list(idxs.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove identical points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get points with zero distance to real (use distance of <0.001 as effectively identical)\n",
    "identical = synth_df['distance_to_closest_real'] < 0.001\n",
    "\n",
    "print (f'Proportion of data points identical to real data points = {identical.mean():0.3f}')\n",
    "# Remove points with zero (or effectively zero) distance to a real data point.  We\n",
    "# do this by setting up a mask that says we only want to see data points where the \"identical\"\n",
    "# criterion we specified above is false (ie they're not identical).  Then we apply that\n",
    "# mask and overwrite our existing synthetic data DataFrame so we've now only got data points\n",
    "# that are not identical to real world data points.\n",
    "mask = identical == False\n",
    "synth_df = synth_df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove points closest to original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of points to remove\n",
    "proportion_to_remove = 0.1\n",
    "\n",
    "# Sort by distance, with highest distances (those we want to keep) at \n",
    "# the top\n",
    "synth_by_distance = synth_df.sort_values(\n",
    "    'distance_to_closest_real', ascending=False)\n",
    "\n",
    "# Limit data.  Calculate the number of entries to keep as being the\n",
    "# total number of synthetic data points we've now got (after having\n",
    "# removed ones identical to real world data points) multiplied by\n",
    "# the proportion we want to keep (the inverse of the proportion to remove).\n",
    "# As we've sorted in descending order by distance, we can then just\n",
    "# use .head to identify how much of the top of list we want to keep\n",
    "# (90% in this case, where we're removing the 10% that are closest - at\n",
    "# the bottom)\n",
    "number_to_keep = int(len(synth_by_distance) * (1 - proportion_to_remove))\n",
    "synth_by_distance = synth_by_distance.head(number_to_keep)\n",
    "\n",
    "# Shuffle and store back in synth_df (frac=1 gives us a sample size of 100%\n",
    "# (ie - all of the ones we said above we wanted to keep))\n",
    "synth_df = synth_by_distance.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show five examples with their closest data points in the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce X_real but with non-standardised (ie the raw original) values for \n",
    "# comparison\n",
    "X_real = np.concatenate([X_train, X_test], axis=0)\n",
    "\n",
    "# Set up Data Frame for comparison\n",
    "comparison = pd.DataFrame(index=X_col_names)\n",
    "\n",
    "# Generate five examples\n",
    "for i in range(5):\n",
    "    # Get synthetic data sample (sample size of 1 - one data point)\n",
    "    sample = synth_df.sample(1)\n",
    "    comparison[f'Synthetic_{i+1}'] = sample[X_col_names].values[0]\n",
    "    # Get closest point from the real data (remember we stored earlier\n",
    "    # the index of the closest real world point, so we can grab it out\n",
    "    # easily here)\n",
    "    closest_id = sample['closest_X_real_row_index']\n",
    "    comparison[f'Synthetic_{i+1}_closest'] = X_real[closest_id, :][0]\n",
    "    \n",
    "# Display the comparisons\n",
    "comparison.round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample from synthetic data to get same size / balance as the original data, and compare counts to ensure identical to original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample from the synthetic data those who had a stroke,\n",
    "# and sample this the same number of times as we had number\n",
    "# who had a stroke in the real data\n",
    "mask = synth_df['stroke'] == 1\n",
    "synth_positive_stroke = synth_df[mask].sample(number_positive_stroke)\n",
    "\n",
    "# The same as above, but for those who didn't have a stroke\n",
    "mask = synth_df['stroke'] == 0\n",
    "synth_negative_stroke = synth_df[mask].sample(number_negative_stroke)\n",
    "\n",
    "# Reconstruct into synth_df and shuffle\n",
    "synth_df = pd.concat([synth_positive_stroke, synth_negative_stroke], axis=0)\n",
    "synth_df = synth_df.sample(frac=1.0, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Number of real data stroke : ', np.sum(data['stroke'] == 1))\n",
    "print ('Number of synthetic data stroke : ', np.sum(synth_df['stroke'] == 1))\n",
    "print ('Number of real data non-stroke : ', np.sum(data['stroke'] == 0))\n",
    "print ('Number of synthetic data non-stroke : ', np.sum(synth_df['stroke'] == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare real and synthetic data using plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,9))\n",
    "\n",
    "# Compare means of patients who didn't have stroke\n",
    "ax1 = fig.add_subplot(221)\n",
    "mask = data['stroke'] == 0\n",
    "x = data[mask][X_col_names].mean()\n",
    "mask = synth_df['stroke'] == 0\n",
    "y = synth_df[mask][X_col_names].mean()\n",
    "ax1.scatter(x,y, alpha=0.5)\n",
    "ax1.plot([0.001, 100],[0.001,100], linestyle='--')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_xlim(1e-3, 1e2)\n",
    "ax1.set_ylim(1e-3, 1e2)\n",
    "ax1.set_xlabel('Original data')\n",
    "ax1.set_ylabel('Synthetic data')\n",
    "ax1.set_title('Non-stroke patients mean')\n",
    "ax1.grid()\n",
    "\n",
    "# Compare means of patients who did have stroke\n",
    "ax2 = fig.add_subplot(222)\n",
    "mask = data['stroke'] == 1\n",
    "x = data[mask][X_col_names].mean()\n",
    "mask = synth_df['stroke'] == 1\n",
    "y = synth_df[mask][X_col_names].mean()\n",
    "ax2.scatter(x,y, alpha=0.5)\n",
    "ax2.plot([0.001, 100],[0.001,100], linestyle='--')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_xlim(1e-3, 1e2)\n",
    "ax2.set_ylim(1e-3, 1e2)\n",
    "ax2.set_xlabel('Original data')\n",
    "ax2.set_ylabel('Synthetic data')\n",
    "ax2.set_title('Stroke patients mean')\n",
    "ax2.grid()\n",
    "\n",
    "# Compare stdevs of patients who didn't have stroke\n",
    "ax3 = fig.add_subplot(223)\n",
    "mask = data['stroke'] == 0\n",
    "x = data[mask][X_col_names].std()\n",
    "mask = synth_df['stroke'] == 0\n",
    "y = synth_df[mask][X_col_names].std()\n",
    "ax3.scatter(x,y, alpha=0.5)\n",
    "ax3.plot([0.001, 100],[0.001,100], linestyle='--')\n",
    "ax3.set_xscale('log')\n",
    "ax3.set_yscale('log')\n",
    "ax3.set_xlim(1e-2, 1e2)\n",
    "ax3.set_ylim(1e-2, 1e2)\n",
    "ax3.set_xlabel('Original data')\n",
    "ax3.set_ylabel('Synthetic data')\n",
    "ax3.set_title('Non-stroke patients StdDevs')\n",
    "ax3.grid()\n",
    "\n",
    "# Compare stdevs of patients who did have stroke\n",
    "ax4 = fig.add_subplot(224)\n",
    "mask = data['stroke'] == 1\n",
    "x = data[mask][X_col_names].std()\n",
    "mask = synth_df['stroke'] == 1\n",
    "y = synth_df[mask][X_col_names].std()\n",
    "ax4.scatter(x,y, alpha=0.5)\n",
    "ax4.plot([0.001, 100],[0.001,100], linestyle='--')\n",
    "ax4.set_xscale('log')\n",
    "ax4.set_yscale('log')\n",
    "ax4.set_xlim(1e-2, 1e2)\n",
    "ax4.set_ylim(1e-2, 1e2)\n",
    "ax4.set_xlabel('Original data')\n",
    "ax4.set_ylabel('Synthetic data')\n",
    "ax4.set_title('Stroke patients StdDevs')\n",
    "ax4.grid()\n",
    "\n",
    "plt.tight_layout(pad=2)\n",
    "plt.savefig('images/stroke_smote_correls.png', facecolor='w', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Logistic Regression model using synthetic data and compare accuracy with model trained on original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X data and standardised\n",
    "X_synth = synth_df[X_col_names]\n",
    "y_synth = synth_df['stroke'].values\n",
    "X_synth_std, X_test_std = standardise_data(X_synth, X_test)\n",
    "\n",
    "# Fit model\n",
    "model_synth = LogisticRegression()\n",
    "model_synth.fit(X_synth_std,y_synth)\n",
    "\n",
    "# Get predictions of test set\n",
    "y_pred_test_synth = model_synth.predict(X_test_std)\n",
    "\n",
    "# Report accuracy\n",
    "accuracy_test_synth = np.mean(y_pred_test_synth == y_test)\n",
    "\n",
    "print (f'Accuracy of predicting test data from model trained on real data = {accuracy_test:0.3f}')\n",
    "print (f'Accuracy of predicting test data from model trained on synthetic data = {accuracy_test_synth:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs = model.predict_proba(X_test_std)[:,1]\n",
    "y_probs_synthetic = model_synth.predict_proba(X_test_std)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "fpr_synth, tpr_synth, thresholds_synth = roc_curve(y_test, y_probs_synthetic)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "roc_auc_snth = auc(fpr_synth, tpr_synth)\n",
    "print (f'ROC AUC real training data: {roc_auc:0.2f}')\n",
    "print (f'ROC AUC synthetic training data: {roc_auc_snth:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "# Plot ROC\n",
    "ax1 = fig.add_subplot()\n",
    "ax1.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('Stroke Data Receiver Operator Characteristic curve')\n",
    "ax1.plot(fpr,tpr, color='green', label = 'Real training data')\n",
    "ax1.plot(fpr_synth,tpr_synth, color='red', label = 'Synthetic training data')\n",
    "text = f'Real data AUC: {roc_auc:.3f}\\nSynthetic data AUC {roc_auc_snth:.3f}'\n",
    "ax1.text(0.52,0.17, text, \n",
    "         bbox=dict(facecolor='white', edgecolor='black'))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig('images/synthetic_roc.png')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
