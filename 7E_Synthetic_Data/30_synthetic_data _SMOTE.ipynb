{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating synthetic Titanic passenger data with SMOTE\n",
    "\n",
    "Synthetic data may be useful to share when it is not possible to share original data.\n",
    "\n",
    "Here we use original Titanic passenger data to create new synthetic passenger data. We then train a logistic model on that synthetic data and test on real data that was not used to create the synthetic data.\n",
    "\n",
    "## Description of SMOTE\n",
    "\n",
    "SMOTE stands for Synthetic Minority Oversampling Technique [1]. SMOTE is more commonly used to create additional data to enhance modelling fitting, especially when one or more classes have low prevalence in the data set. Hence the description of oversampling.\n",
    "\n",
    "SMOTE works by finding near-neighbor points in the original data, and creating new data points from interpolating between two near-neighbor points.\n",
    "\n",
    "Here we remove the real data used to create the synthetic data, leaving only the synthetic data. After generating synthetic data we remove any data points that, by chance, are identical to original real data points, and also remove 10% of points that are closest to the original data points. We measure ‘closeness’ by the Cartesian distance between standardised data values.\n",
    "\n",
    "![](./images/smote.png)\n",
    "\n",
    "*Demonstration of SMOTE method. (a) Data points with two features (shown on x and y axes) are represented. Points are colour-coded by class label. (b) A data point from a class is picked at random, shown by the black point, and then the closest neighbours of the same class are identified, as shown by yellow points. Here we show 3 closest neighbours, but the default in the SMOTE `Imbalanced-Learn` library is 6. One of those near-neighbour points is selected at random (shown by the second black point). A new data point, shown in red, is created at a random distance between the two selected data points.*\n",
    "\n",
    "### Handling integer, binary, and categorical data\n",
    "\n",
    "The standard SMOTE method generates floating point non-integer) values between data points. There are alternative ways of handing integer, binary, and categorical data using the SMOTE method. Here the methods we use are:\n",
    "\n",
    "* *Integer* values: Round the resulting synthetic data point value to the closest integer.\n",
    "\n",
    "* *Binary*: Code the value as 0 or 1, and round the resulting synthetic data point value to the closest integer (0 or 1).\n",
    "\n",
    "* *Categorical*: One-hot encode the categorical feature. Generate the synthetic data for each category value. Identify the category with the highest value and set to 1 while setting all others to 0.\n",
    "\n",
    "### Implementation with IMBLEARN\n",
    "\n",
    "Here use the implementation in the IMBLEARN IMBALANCED-LEARN [2] \n",
    "\n",
    "[1] Chawla, N.V., Bowyer, K.W., Hall, L.O., Kegelmeyer, W.P. “SMOTE: Synthetic minority over-sampling technique,” Journal of Artificial Intelligence Research, vol. 16, pp. 321-357, 2002.\n",
    "\n",
    "[2] Lemaitre, G., Nogueira, F. and Aridas, C. (2016), Imbalanced-learn: A Python Toolbox to Tackle the Curse of Imbalanced Datasets in Machine Learning. arXiv:1609.06570 (https://pypi.org/project/imbalanced-learn/, `pip install imbalanced-learn`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of code sections\n",
    "\n",
    "Below is an outline of this notebook.\n",
    "\n",
    "1. Load packages\n",
    "1. Load and process data\n",
    "    1. Load data\n",
    "    1. Divide into X (features) and y (labels)\n",
    "    1. Divide into training and test sets\n",
    "    1. Standardise data\n",
    "1. Fit and test logistic regression model on real data\n",
    "    1. Fit model\n",
    "    1. Predict values\n",
    "    1. Calculate accuracy\n",
    "1. Make Synthetic data\n",
    "    1. Function to create synethetic data\n",
    "    1. Generate raw synthetic data\n",
    "    1. Processing of raw synthetic data\n",
    "        1. Prepare lists of categorical, integer, and binary features\n",
    "        1. Function to process raw synthetic categorical data to one-hot encoded\n",
    "        1. Process raw synthetic data\n",
    "    1. Remove synthetic data that is a duplication of original data or close to original data\n",
    "        1. Remove identical points\n",
    "        1. Remove closest points to original data\n",
    "    1. Show five examples with their closest data points in the original data\n",
    "    1. Sample from synthetic data to get same size/balance as the original data\n",
    "1. Comparison of real and synthetic data\n",
    "1. Test synthetic data for training a logistic regression model\n",
    "    1. Fit model using synthetic data and check accuracy\n",
    "    1. Receiver Operator Characteristic curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import machine learning methods\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "# Import package for SMOTE\n",
    "import imblearn\n",
    "\n",
    "# Turn warnings off to keep notebook clean\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and process data\n",
    "\n",
    "### Load data\n",
    "\n",
    "The section below downloads pre-processed data, and saves it to a subfolder (from where this code is run).\n",
    "If data has already been downloaded that cell may be skipped.\n",
    "\n",
    "Code that was used to pre-process the data ready for machine learning may be found at:\n",
    "https://michaelallen1966.github.io/titanic/01_preprocessing.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_required = True\n",
    "\n",
    "if download_required:\n",
    "    \n",
    "    # Download processed data:\n",
    "    address = 'https://raw.githubusercontent.com/MichaelAllen1966/' + \\\n",
    "                '1804_python_healthcare/master/titanic/data/processed_data.csv'\n",
    "    \n",
    "    data = pd.read_csv(address)\n",
    "\n",
    "    # Create a data subfolder if one does not already exist\n",
    "    import os\n",
    "    data_directory ='./data/'\n",
    "    if not os.path.exists(data_directory):\n",
    "        os.makedirs(data_directory)\n",
    "\n",
    "    # Save data\n",
    "    data.to_csv(data_directory + 'processed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data/processed_data.csv')\n",
    "# Make all data 'float' type and drop Passenger ID\n",
    "data = data.astype(float)\n",
    "data.drop('PassengerId', axis=1, inplace=True) # Remove passenger ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record number in each class\n",
    "number_died = np.sum(data['Survived'] == 0)\n",
    "number_survived = np.sum(data['Survived'] == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide into X (features) and y (labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Survived',axis=1) # X = all 'data' except the 'survived' column\n",
    "y = data['Survived'] # y = 'survived' column from 'data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divide into training and test sets\n",
    "\n",
    "To demonstrate the method we will use a single train/test split for simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show examples from the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standardise data\n",
    "\n",
    "To standardise the data we subtract the mean of the training set values, and divide by the standard deviation of the training data. Note that the mean and standard deviation of the training data are used to standardise the test set data as well. Here we use sklearn's `StandardScaler` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardise_data(X_train, X_test):\n",
    "    \n",
    "    # Initialise a new scaling object for normalising input data\n",
    "    sc = StandardScaler() \n",
    "\n",
    "    # Set up the scaler just on the training set\n",
    "    sc.fit(X_train)\n",
    "\n",
    "    # Apply the scaler to the training and test sets\n",
    "    train_std=sc.transform(X_train)\n",
    "    test_std=sc.transform(X_test)\n",
    "    \n",
    "    return train_std, test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_std, X_test_std = standardise_data(X_train, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and test logistic regression model on real data\n",
    "\n",
    "### Fit model\n",
    "\n",
    "We will fit a logistic regression model, using sklearn's `LogisticRegression` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression()\n",
    "model.fit(X_train_std,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict values\n",
    "\n",
    "Now we can use the trained model to predict survival. We will test the accuracy of both the training and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict training and test set labels\n",
    "y_pred_train = model.predict(X_train_std)\n",
    "y_pred_test = model.predict(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy\n",
    "\n",
    "Here we measure accuracy simply as the proportion of passengers where we make the correct prediction (later we will use Receiver Operator Characteristic curves for a more thorough analysis)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_train = np.mean(y_pred_train == y_train)\n",
    "accuracy_test = np.mean(y_pred_test == y_test)\n",
    "\n",
    "print (f'Accuracy of predicting training data = {accuracy_train:0.3f}')\n",
    "print (f'Accuracy of predicting test data = {accuracy_test:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Synthetic data\n",
    "\n",
    "### Function to create synthetic data\n",
    "\n",
    "This function is to make synethetic data for two classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_synthetic_data_smote(X, y, number_of_samples=[1000,1000]):\n",
    "    \"\"\"\n",
    "    Synthetic data generation for two classes.\n",
    "        \n",
    "    Inputs\n",
    "    ------\n",
    "    original_data: X, y numpy arrays (y should have label 0 and 1)\n",
    "    number_of_samples: number of samples to generate (list for y=0, y=1)\n",
    "    (Note - number_of_samples has default of 1000 samples for each class\n",
    "    if no numbers are specified at the point of calling the function)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X_synthetic: NumPy array\n",
    "    y_synthetic: NumPy array\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    # import SMOTE from imblearn so we can use it\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    \n",
    "    # Count instances in each class\n",
    "    count_label_0 = np.sum(y==0)\n",
    "    count_label_1 = np.sum(y==1)\n",
    "    \n",
    "    # SMOTE requires final class counts; add current counts to required counts\n",
    "    # (which are passed into the function)\n",
    "    n_class_0 = number_of_samples[0] + count_label_0\n",
    "    n_class_1 = number_of_samples[1] + count_label_1\n",
    "\n",
    "    # Use SMOTE to sample data points.  The number of points that we pass over\n",
    "    # to SMOTE is calculated above (the number of synthetic data samples we\n",
    "    # want, which we passed into the function + the counts from the original\n",
    "    # data).  This tells SMOTE how many TOTAL data points are needed (original\n",
    "    # + synthetic) for each class.  It then uses the original data to generate\n",
    "    # new synthetic data points.\n",
    "    # For example, imagine our original data has 100 samples for class 0 and 50\n",
    "    # for class 1, and we tell SMOTE we want 100 synthetic data points for \n",
    "    # class 0 and 150 synthetic data points for class 1.  We tell SMOTE that we\n",
    "    # need a total of 200 data points for class 0 (100 original + 100 synthetic)\n",
    "    # and 200 data points for class 1 (50 original + 150 synthetic).  It will\n",
    "    # then fill those data points by taking the original data (which will fill\n",
    "    # up the first 100 \"slots\" for class 0, and the first 50 \"slots\" for class 1)\n",
    "    # and then use these original data points to sample new synthetic data points\n",
    "    # to fill the remaining \"slots\" in each class.\n",
    "    X_resampled, y_resampled = SMOTE(\n",
    "        sampling_strategy = {0:n_class_0, 1:n_class_1}).fit_resample(X, y)\n",
    "\n",
    "    # Get just the additional (synthetic) data points.  By using len(X) for the\n",
    "    # X (input feature) data, and len(y) for the y (output label) data, we skip\n",
    "    # the original data, and just start from the newly created synthetic data,\n",
    "    # generated by SMOTE (above)\n",
    "    X_synthetic = X_resampled[len(X):]\n",
    "    y_synthetic = y_resampled[len(y):]\n",
    "                                                                   \n",
    "    return X_synthetic, y_synthetic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will generate twice as much raw synthetic data for each class as the current data has. This will allow us to remove points that are identical to, or close to, original data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate raw synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get counts of classes from y_train\n",
    "unique, original_frequency = np.unique(y_train, return_counts = True)\n",
    "required_smote_count = list(original_frequency * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function we wrote above to generate and extract the synthetic data\n",
    "X_synthetic, y_synthetic = make_synthetic_data_smote(\n",
    "        X_train, y_train, number_of_samples=required_smote_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing of raw synthetic data\n",
    "\n",
    "#### Prepare lists of categorical, integer, and binary features\n",
    "\n",
    "These lists will be used to process raw synthetic data to the appropriate data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get full list of column names (the names of our features)\n",
    "X_col_names = list(X_train)\n",
    "\n",
    "# Set categorical one-hots cols using common prefix \n",
    "# First, let's set up the categorical columns, which we'll need to\n",
    "# \"one hot encode\".  We've got two categorical features in the\n",
    "# Titanic data - where they embarked, and their cabin letter.\n",
    "# Here, we'll use some code to grab out all the categorical columns\n",
    "# (remember, they're set up to be one hot encoded in the original data,\n",
    "# so if there are three places from which a passenger can embark, then\n",
    "# there are three columns for the embarked feature, and one of them will\n",
    "# have a 1 value, whilst the others will have a 0 value.).\n",
    "# We do this here by giving the common prefix (start of the name) of the\n",
    "# columns we want, and then use a list comprehension to find all column\n",
    "# names that start with that prefix, and store those in a list of one hot\n",
    "# columns.  Remember, strings (such as the names of columns here) can be\n",
    "# treated as lists of characters (so x[0] would give the first character)\n",
    "# The list comprehension code below may look confusing initially, but it\n",
    "# basically says \"give me the column name if it starts with \"Embarked_\" or\n",
    "# \"CabinLetter_\"\n",
    "categorical = ['Embarked_', 'CabinLetter_']\n",
    "one_hot_cols = []\n",
    "for col in categorical:\n",
    "    one_hot_cols.append([x for x in X_col_names if x[0:len(col)] == col])\n",
    "    \n",
    "# Set integer columns\n",
    "integer_cols = ['Pclass',\n",
    "                'Age',\n",
    "                'Parch',\n",
    "                'Fare',\n",
    "                'SibSp',\n",
    "                'CabinNumber']\n",
    "\n",
    "# Set binary columns\n",
    "binary_cols = ['male',\n",
    "               'AgeImputed',\n",
    "               'EmbarkedImputed',\n",
    "               'CabinNumberImputed']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to process raw synthetic categorical data to one-hot encoded\n",
    "\n",
    "Sets highest value to 1 and all others to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_one_hot(x):\n",
    "    \"\"\"\n",
    "    Takes a list/array/series and turns it into a one-hot encoded\n",
    "    list/array series, by setting 1 for highest value and 0 for all \n",
    "    others\n",
    "    \n",
    "    \"\"\"\n",
    "    # Get argmax (this returns the index of the highest values in\n",
    "    # the list / array / series passed in to the function)\n",
    "    highest = np.argmax(x)\n",
    "    # Set all values to zero (just multiply all values by 0)\n",
    "    x *= 0.0\n",
    "    # Set the value that was found to be the highest to 1, by\n",
    "    # using the index we found using argmax above\n",
    "    x[highest] = 1.0\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Process raw synthetic data:\n",
    "\n",
    "1. Transfer data to a DataFrame and add column names\n",
    "1. Process one-hot categorical data fields\n",
    "1. Process integer data fields\n",
    "1. Process binary data fields\n",
    "1. Add *y* data with label\n",
    "1. Shuffle data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set y_label\n",
    "y_label = 'Survived'\n",
    "\n",
    "# Create a data frame with id to store the synthetic data\n",
    "synth_df = pd.DataFrame()\n",
    "\n",
    "# Transfer X values to the new DataFrame\n",
    "synth_df=pd.concat([synth_df, \n",
    "                    pd.DataFrame(X_synthetic, columns=X_col_names)],\n",
    "                    axis=1)\n",
    "\n",
    "# Make columns (that need to be) one hot encoded using the\n",
    "# function we wrote above, using the raw synthetic data\n",
    "for one_hot_list in one_hot_cols:    \n",
    "    for index, row in synth_df.iterrows():\n",
    "        x = row[one_hot_list]\n",
    "        x_one_hot = make_one_hot(x)\n",
    "        row[x_one_hot.index]= x_one_hot.values\n",
    "\n",
    "# Make integer as necessary by rounding the raw synthetic data\n",
    "for col in integer_cols:\n",
    "    synth_df[col] = synth_df[col].round(0)\n",
    "\n",
    "# Round binary cols and clip so values under 0 or above 1\n",
    "# are set to 0 and 1 respectively (this won't happen with\n",
    "# SMOTE, as it will only sample between the two points (so \n",
    "# points sampled between binary points will always be \n",
    "# between 0 and 1) but it can happen with other methods)\n",
    "for col in binary_cols:\n",
    "    synth_df[col] = np.clip(synth_df[col],0,1).round(0)\n",
    "    \n",
    "# Add y data with a label\n",
    "y_list = list(y_synthetic)\n",
    "synth_df[y_label] = y_list\n",
    "\n",
    "# Shuffle data\n",
    "synth_df = synth_df.sample(frac=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show sample of synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove synthetic data that is a duplication of original data or close to original data\n",
    "\n",
    "For each synthetic data point find nearest neighbour in the real data set (based on Cartesian distance of standardised data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardise synthetic data (based on real training data)\n",
    "X_train_std, X_synth_std = standardise_data(X_train, X_synthetic)\n",
    "\n",
    "# Get ALL real X data (combine standardised training + test data)\n",
    "# We do this because we need to check for duplicates / very close\n",
    "# values in all of the real data we've got\n",
    "X_real_std = np.concatenate([X_train_std, X_test_std], axis=0)\n",
    "  \n",
    "# Use SciKitLearn neighbors.NearestNeighbors to find nearest neighbour\n",
    "# to each data point. First, we fit to the real standardised data \n",
    "# (all of it, train + test set).  Then we can give it the synthetic data\n",
    "# and ask it to give us the cartesian distance and ID of its nearest\n",
    "# real world data point neighbour for each synthetic data point.\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nn = NearestNeighbors(n_neighbors=1, algorithm='auto').fit(X_real_std)\n",
    "dists, idxs = nn.kneighbors(X_synth_std)\n",
    "\n",
    "# Store the index and ids (indices) in the synthetic data DataFrame\n",
    "# Flatten just reduces something in more than 1 dimension down to\n",
    "# 1 dimension (eg a list of lists becomes a single list)\n",
    "synth_df['distance_to_closest_real'] = list(dists.flatten())\n",
    "synth_df['closest_X_real_row_index'] = list(idxs.flatten())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a peek at our synthetic data.  Observe the two new columns on the far right (scroll across), which now tells us how close the nearest neighbouring real world data point is (along with it's id (index) so we can look it up) for each synthetic data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "synth_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove identical points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get points with zero distance to real (use distance of <0.001 as effectively identical)\n",
    "identical = synth_df['distance_to_closest_real'] < 0.001\n",
    "\n",
    "print (f'Proportion of data points identical to real data points = {identical.mean():0.3f}')\n",
    "# Remove points with zero (or effectively zero) distance to a real data point.  We\n",
    "# do this by setting up a mask that says we only want to see data points where the \"identical\"\n",
    "# criterion we specified above is false (ie they're not identical).  Then we apply that\n",
    "# mask and overwrite our existing synthetic data DataFrame so we've now only got data points\n",
    "# that are not identical to real world data points.\n",
    "mask = identical == False\n",
    "synth_df = synth_df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Remove closest points to original data\n",
    "\n",
    "Remove 10% of points that are closest to original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion of points to remove\n",
    "proportion_to_remove = 0.1\n",
    "\n",
    "# Sort by distance, with highest distances (those we want to keep) at \n",
    "# the top\n",
    "synth_by_distance = synth_df.sort_values(\n",
    "    'distance_to_closest_real', ascending=False)\n",
    "\n",
    "# Limit data.  Calculate the number of entries to keep as being the\n",
    "# total number of synthetic data points we've now got (after having\n",
    "# removed ones identical to real world data points) multiplied by\n",
    "# the proportion we want to keep (the inverse of the proportion to remove).\n",
    "# As we've sorted in descending order by distance, we can then just\n",
    "# use .head to identify how much of the top of list we want to keep\n",
    "# (90% in this case, where we're removing the 10% that are closest - at\n",
    "# the bottom)\n",
    "number_to_keep = int(len(synth_by_distance) * (1 - proportion_to_remove))\n",
    "synth_by_distance = synth_by_distance.head(number_to_keep)\n",
    "\n",
    "# Shuffle and store back in synth_df (frac=1 gives us a sample size of 100%\n",
    "# (ie - all of the ones we said above we wanted to keep))\n",
    "synth_df = synth_by_distance.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show five examples with their closest data points in the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reproduce X_real but with non-standardised (ie the raw original) values for \n",
    "# comparison\n",
    "X_real = np.concatenate([X_train, X_test], axis=0)\n",
    "\n",
    "# Set up Data Frame for comparison\n",
    "comparison = pd.DataFrame(index=X_col_names)\n",
    "\n",
    "# Generate five examples\n",
    "for i in range(5):\n",
    "    # Get synthetic data sample (sample size of 1 - one data point)\n",
    "    sample = synth_df.sample(1)\n",
    "    comparison[f'Synthetic_{i+1}'] = sample[X_col_names].values[0]\n",
    "    # Get closest point from the real data (remember we stored earlier\n",
    "    # the index of the closest real world point, so we can grab it out\n",
    "    # easily here)\n",
    "    closest_id = sample['closest_X_real_row_index']\n",
    "    comparison[f'Synthetic_{i+1}_closest'] = X_real[closest_id, :][0]\n",
    "    \n",
    "# Display the comparisons\n",
    "comparison.round(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample from synthetic data to get same size/balance as the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample from the synthetic data those who died,\n",
    "# and sample this the same number of times as we had number\n",
    "# who died in the real data\n",
    "mask = synth_df['Survived'] == 0\n",
    "synth_died = synth_df[mask].sample(number_died)\n",
    "\n",
    "# The same as above, but for those who survived\n",
    "mask = synth_df['Survived'] == 1\n",
    "synth_survived = synth_df[mask].sample(number_survived)\n",
    "\n",
    "# Reconstruct into synth_df and shuffle\n",
    "synth_df = pd.concat([synth_died, synth_survived], axis=0)\n",
    "synth_df = synth_df.sample(frac=1.0, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare counts with original data.  These should be identical for real vs synthetic if the above has worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Number of real data survived: ', np.sum(data['Survived'] == 1))\n",
    "print ('Number of synthetic data survived: ', np.sum(synth_df['Survived'] == 1))\n",
    "print ('Number of real data died: ', np.sum(data['Survived'] == 0))\n",
    "print ('Number of synthetic data died: ', np.sum(synth_df['Survived'] == 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of real and synthetic data\n",
    "\n",
    "The charts below compare means and standard deviations of real and synthetic data, comparing passengers who survived or died separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(9,9))\n",
    "\n",
    "# Compare means of patients who died\n",
    "ax1 = fig.add_subplot(221)\n",
    "mask = data['Survived'] == 0\n",
    "x = data[mask][X_col_names].mean()\n",
    "mask = synth_df['Survived'] == 0\n",
    "y = synth_df[mask][X_col_names].mean()\n",
    "ax1.scatter(x,y, alpha=0.5)\n",
    "ax1.plot([0.001, 100],[0.001,100], linestyle='--')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_yscale('log')\n",
    "ax1.set_xlim(1e-3, 1e2)\n",
    "ax1.set_ylim(1e-3, 1e2)\n",
    "ax1.set_xlabel('Original data')\n",
    "ax1.set_ylabel('Synthetic data')\n",
    "ax1.set_title('Non-survivors mean')\n",
    "ax1.grid()\n",
    "\n",
    "# Compare means of patients who survived\n",
    "ax2 = fig.add_subplot(222)\n",
    "mask = data['Survived'] == 1\n",
    "x = data[mask][X_col_names].mean()\n",
    "mask = synth_df['Survived'] == 1\n",
    "y = synth_df[mask][X_col_names].mean()\n",
    "ax2.scatter(x,y, alpha=0.5)\n",
    "ax2.plot([0.001, 100],[0.001,100], linestyle='--')\n",
    "ax2.set_xscale('log')\n",
    "ax2.set_yscale('log')\n",
    "ax2.set_xlim(1e-3, 1e2)\n",
    "ax2.set_ylim(1e-3, 1e2)\n",
    "ax2.set_xlabel('Original data')\n",
    "ax2.set_ylabel('Synthetic data')\n",
    "ax2.set_title('Survivors mean')\n",
    "ax2.grid()\n",
    "\n",
    "# Compare stdevs of patients who died\n",
    "ax3 = fig.add_subplot(223)\n",
    "mask = data['Survived'] == 0\n",
    "x = data[mask][X_col_names].std()\n",
    "mask = synth_df['Survived'] == 0\n",
    "y = synth_df[mask][X_col_names].std()\n",
    "ax3.scatter(x,y, alpha=0.5)\n",
    "ax3.plot([0.001, 100],[0.001,100], linestyle='--')\n",
    "ax3.set_xscale('log')\n",
    "ax3.set_yscale('log')\n",
    "ax3.set_xlim(1e-2, 1e2)\n",
    "ax3.set_ylim(1e-2, 1e2)\n",
    "ax3.set_xlabel('Original data')\n",
    "ax3.set_ylabel('Synthetic data')\n",
    "ax3.set_title('Non-survivors StdDevs')\n",
    "ax3.grid()\n",
    "\n",
    "# Compare stdevs of patients who survived\n",
    "ax4 = fig.add_subplot(224)\n",
    "mask = data['Survived'] == 1\n",
    "x = data[mask][X_col_names].std()\n",
    "mask = synth_df['Survived'] == 1\n",
    "y = synth_df[mask][X_col_names].std()\n",
    "ax4.scatter(x,y, alpha=0.5)\n",
    "ax4.plot([0.001, 100],[0.001,100], linestyle='--')\n",
    "ax4.set_xscale('log')\n",
    "ax4.set_yscale('log')\n",
    "ax4.set_xlim(1e-2, 1e2)\n",
    "ax4.set_ylim(1e-2, 1e2)\n",
    "ax4.set_xlabel('Original data')\n",
    "ax4.set_ylabel('Synthetic data')\n",
    "ax4.set_title('Survivors StdDevs')\n",
    "ax4.grid()\n",
    "\n",
    "plt.tight_layout(pad=2)\n",
    "plt.savefig('images/smote_correls.png', facecolor='w', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test synthetic data for training a logistic regression model\n",
    "\n",
    "Note that we create synthetic data using the training portion of our orginal train/test split. We then test the model on the original test data. The data used to create synthetic data is not present in the test data (this would cause leakage of test data into the training data and over-estimate performance).\n",
    "\n",
    "### Fit model using synthetic data and check accuracy\n",
    "\n",
    "Now let's use our synthetic data to train our Logistic Regression model, and compare performance on the one trained with the real data, and the one trained on synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get X data and standardised\n",
    "X_synth = synth_df[X_col_names]\n",
    "y_synth = synth_df['Survived'].values\n",
    "X_synth_std, X_test_std = standardise_data(X_synth, X_test)\n",
    "\n",
    "# Fit model\n",
    "model_synth = LogisticRegression()\n",
    "model_synth.fit(X_synth_std,y_synth)\n",
    "\n",
    "# Get predictions of test set\n",
    "y_pred_test_synth = model_synth.predict(X_test_std)\n",
    "\n",
    "# Report accuracy\n",
    "accuracy_test_synth = np.mean(y_pred_test_synth == y_test)\n",
    "\n",
    "print (f'Accuracy of predicting test data from model trained on real data = {accuracy_test:0.3f}')\n",
    "print (f'Accuracy of predicting test data from model trained on synthetic data = {accuracy_test_synth:0.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Receiver Operator Characteristic curves\n",
    "\n",
    "Now let's generate our ROC curves (refer back to your notes from session 7C : Machine Learning - Who Would Survive the Titanic?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probs = model.predict_proba(X_test_std)[:,1]\n",
    "y_probs_synthetic = model_synth.predict_proba(X_test_std)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr, tpr, thresholds = roc_curve(y_test, y_probs)\n",
    "fpr_synth, tpr_synth, thresholds_synth = roc_curve(y_test, y_probs_synthetic)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "roc_auc_snth = auc(fpr_synth, tpr_synth)\n",
    "print (f'ROC AUC real training data: {roc_auc:0.2f}')\n",
    "print (f'ROC AUC synthetic training data: {roc_auc_snth:0.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "\n",
    "# Plot ROC\n",
    "ax1 = fig.add_subplot()\n",
    "ax1.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "ax1.set_xlabel('False Positive Rate')\n",
    "ax1.set_ylabel('True Positive Rate')\n",
    "ax1.set_title('Titanic survival Receiver Operator Characteristic curve')\n",
    "ax1.plot(fpr,tpr, color='green', label = 'Real training data')\n",
    "ax1.plot(fpr_synth,tpr_synth, color='red', label = 'Synthetic training data')\n",
    "text = f'Real data AUC: {roc_auc:.3f}\\nSynthetic data AUC {roc_auc_snth:.3f}'\n",
    "ax1.text(0.52,0.17, text, \n",
    "         bbox=dict(facecolor='white', edgecolor='black'))\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.savefig('images/synthetic_roc.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "Here we have used the SMOTE method to create synthetic data. We have removed any data points that are identical to the original data, and have also removed 10% of synthetic data points that are closest to original data.\n",
    "\n",
    "Mean and standard deviations of the synthetic data are very similar to the original data. \n",
    "\n",
    "Synthetic data trains a logistic regression model with minimal loss of accuracy when compared with training with original data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
